{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Data Preperation\n",
    "\n",
    "Usually we have more notebooks than submissions which are optional. We dentote this in the filename and header. Additionally, they contain no final score at the end.\n",
    "\n",
    "In the following notebooks, we will supply you with all neccessary tools so you can concentrate on writing the model specific parts as this is the main focus of this class. However, you should also be proficient with handling data and how to prepare it for your specific task. In fact, most of the jobs that involve deep learning in industry are very data related so this is an important skill that you have to pick up.\n",
    "\n",
    "For this, you should consider performing the following steps using numpy comprehensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start a classification task you should *always* take a look at some samples of your dataset. This way you can make sure that the data input has worked as intended and you can get a feeling for the dataset. Load the CIFAR-10 data and visualize a subset of the images (X) in the CIFAR-10 dataset in this notebook or a seperate file (see the previous notebook how to import files and read data) and identify all ten classes (y). (Hint: Use matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your visualization code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training data into there sets: a \"training\", \"validation\" and \"test\" consisting of 48000, 1000 and 1000 elements. Numpy makes this very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your split code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing: compute the mean image and substract this image from the images of our datasets. What would you do with the validation and test set? Why is this step helpful for machine learning methods? Research other preprocessing methods that might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and substract mean image from all datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
